Выполненная работа:
	1) Спроектирована структура хранилища DWH. Оно состоит из Staging-слоя, DDS и CDM.
	2) Реализован staging-слой. В него попадают данные из системы-источника в исходном формате данных. 
	   Он состоит из таблиц apisystem_couriers, apisystem_deliveries, а также вспомогательной таблицы loads_check, хранящей результаты работы загрузок из источника.
	   Код создания таблиц: "\de-project-sprint-5\sql_scripts\create_stg.sql", "\de-project-sprint-5\sql_scripts\create_stg_loads_check.sql"
	3) Реализован DDS-слой. В него попадают данные из staging-слоя, приведённые к необходимым для витрины типам данных.
	   Он состоит из таблиц dm_couriers, dm_timestamps и dm_deliveries. Эти таблицы образуют модель звезда с таблицей фактов dm_deliveries.
	   Код создания таблиц: "\de-project-sprint-5\sql_scripts\create_dds.sql"
	4) Реализован CDM-слой. В него попадают агрегированные данные из DDS-слоя.
	   Он состоит из витрины dm_courier_ledger, содержащей данные по выплатам курьерам, разбитым по месяцам.
	   Код создания таблиц: "\de-project-sprint-5\sql_scripts\create_cdm.sql"
	5) Реализован ETL-процесс, который подключается к системе-источнику по api. Процесс загрузки данных из источника в staging-слой разрабатываемой системы реализован с помощью DAG-оркестратора Airflow 
	   на языке программирования Python. DAG содержит задачи на соответствующую загрузку данных из источника. 
	   Для реализации этого процесса были использованы следующие библиотеки Python: requests, pandas, json, logging, datetime, airflow. 
	   Исходный код прилагается: "\de-project-sprint-5\src\dags\couriers_get_dag.py", "\de-project-sprint-5\src\dags\deliveries_get_dag.py"
	6) Реализован ETL-процесс, который переносит данные из stging-слоя в dds-слой и приводит их к необходимым типам данных.
	   Процесс загрузки данных из staging-слоя в dds-слой разрабатываемой системы реализован с помощью DAG-оркестратора Airflow 
	   на языке программирования Python. DAG содержит задачи на соответствующую загрузку данных.
	   Для реализации этого процесса были использованы следующие библиотеки Python:  logging, datetime, airflow. 
	   Исходный код прилагается: "\de-project-sprint-5\src\dags\stg_to_dds_dag.py"
	   Код sql запросов хранится в файлах: "\de-project-sprint-5\sql_scripts\insert_couriers.sql", "\de-project-sprint-5\sql_scripts\insert_timestamps.sql", "\de-project-sprint-5\sql_scripts\insert_deliveries.sql"
	7) Реализован ETL-процесс, который переносит данные из dds-слоя в cdm-слой и аггрегирует их.
	   Процесс загрузки данных из dds-слоя в cdm-слой разрабатываемой системы реализован с помощью DAG-оркестратора Airflow 
	   на языке программирования Python. DAG содержит задачи на соответствующую загрузку данных.
	   Для реализации этого процесса были использованы следующие библиотеки Python:  logging, datetime, airflow. 
	   Исходный код прилагается: "\de-project-sprint-5\src\dags\dds_to_cdm_dag.py"
	   Код sql запросов хранится в файле: "\de-project-sprint-5\sql_scripts\insert_ledger.sql"

Выполнение требований подтверждается результатами тестирования:
	1) Проведены позитивные тесты таблиц dds и cdm слоёв на отсутсвие дублей и пустых значений. Все тесты пройдены успешно.
	   Код тестов хранится в файле: "\de-project-sprint-5\sql_scripts\positive_tests.sql"
	2) Проведены негативные тесты на реагирование системы на отсутствие в данных системы источника как обязательных, так и необязательных полей. Все тесты пройдены успешно.
	   Коды тестов хранится в файлах: "\de-project-sprint-5\sql_scripts\negative_couriers_test.sql", "\de-project-sprint-5\sql_scripts\negative_timestamps_test.sql", "\de-project-sprint-5\sql_scripts\negative_deliveries_test.sql"